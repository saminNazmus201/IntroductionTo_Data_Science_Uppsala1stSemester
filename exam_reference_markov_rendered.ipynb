{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377a5a78",
   "metadata": {},
   "source": [
    "# Exam Reference \u2014 1MS041 (copy\u2011paste ready)\n",
    "**What this is:** compact \u201ctoolbox + mini\u2011textbook\u201d for the exam: definitions, formulas (plain + LaTeX), and *ready-to-copy* Python templates.  \n",
    "**What this is NOT:** solutions to any specific past exam.\n",
    "\n",
    "**How to use in the exam**\n",
    "- Jump to the relevant chapter.\n",
    "- Copy the code block, then only edit: data arrays, distribution functions, matrix values.\n",
    "- Keep comments: they help you earn partial credit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f001f56",
   "metadata": {},
   "source": [
    "## Quick index\n",
    "1. **Sampling + Monte Carlo integration + Hoeffding CI**\n",
    "2. **Logistic regression + calibration + 0\u20131 loss + CI**\n",
    "3. **Markov chains** (transition matrix, irreducible, periodic, stationary, reversible)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b96e9d",
   "metadata": {},
   "source": [
    "# 1) Sampling + Monte Carlo integration (MC)\n",
    "\n",
    "## 1.1 Definitions (summary like a textbook)\n",
    "\n",
    "### Random variable generation\n",
    "Goal: generate i.i.d. samples \\(X_1,\\dots,X_n\\) from a target distribution with CDF \\(F\\) or density \\(f\\).\n",
    "\n",
    "### Inverse transform sampling\n",
    "If \\(F^{-1}\\) is available:\n",
    "- draw \\(U\\sim \\mathrm{Unif}(0,1)\\)\n",
    "- set \\(X = F^{-1}(U)\\)\n",
    "Then \\(X\\) has CDF \\(F\\).\n",
    "\n",
    "### Rejection sampling (accept\u2013reject)\n",
    "Use when you can evaluate \\(f(x)\\) (possibly up to a constant) but cannot sample directly.\n",
    "\n",
    "Pick:\n",
    "- proposal density \\(g(x)\\) that you can sample from,\n",
    "- constant \\(M\\) such that \\(f(x) \\le M g(x)\\) for all \\(x\\) in support.\n",
    "\n",
    "Algorithm:\n",
    "1. Sample \\(Y\\sim g\\).\n",
    "2. Sample \\(U\\sim \\mathrm{Unif}(0,1)\\).\n",
    "3. Accept \\(Y\\) if \\(U \\le f(Y)/(M g(Y))\\); else reject and repeat.\n",
    "\n",
    "Acceptance rate is about \\(1/M\\). Choosing a good \\(g\\) is key.\n",
    "\n",
    "### Monte Carlo integration\n",
    "To approximate \\(I=\\mathbb{E}[h(X)]\\) with samples \\(X_i\\sim f\\):\n",
    "\\[\n",
    "\\hat I = \\frac{1}{n}\\sum_{i=1}^n h(X_i).\n",
    "\\]\n",
    "Importance sampling with \\(Y_i\\sim g\\):\n",
    "\\[\n",
    "I = \\int h(x) f(x)\\,dx = \\mathbb{E}_g\\!\\left[h(Y)\\frac{f(Y)}{g(Y)}\\right],\n",
    "\\quad\n",
    "\\hat I = \\frac{1}{n}\\sum_{i=1}^n h(Y_i)\\frac{f(Y_i)}{g(Y_i)}.\n",
    "\\]\n",
    "\n",
    "### Hoeffding inequality (bounded variables)\n",
    "If \\(Z_i\\in[a,b]\\) i.i.d. and \\(\\hat\\mu=\\frac{1}{n}\\sum Z_i\\):\n",
    "\\[\n",
    "\\Pr(|\\hat\\mu-\\mu|\\ge \\varepsilon)\\le 2\\exp\\!\\left(\\frac{-2n\\varepsilon^2}{(b-a)^2}\\right).\n",
    "\\]\n",
    "CI:\n",
    "\\[\n",
    "\\hat\\mu \\pm (b-a)\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}.\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Copyable formulas (plain text)\n",
    "- Inverse sampling: U ~ Unif(0,1), X = F^{-1}(U).\n",
    "- Reject sampling: accept Y~g with prob f(Y)/(M*g(Y)).\n",
    "- MC: I \u2248 (1/n) \u03a3 h(X_i).\n",
    "- Importance: I = int h(x) f(x) dx \u2248 (1/n) \u03a3 h(Y_i) * f(Y_i)/g(Y_i).\n",
    "- Hoeffding CI: mean \u00b1 (b-a)*sqrt(log(2/delta)/(2n)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d96e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Imports for this chapter\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c93514",
   "metadata": {},
   "source": [
    "## 1.4 Code templates (copy\u2011paste)\n",
    "\n",
    "### A) Rejection sampler (vectorized)\n",
    "Fill in:\n",
    "- g_sampler(size)\n",
    "- f_unnorm(x)\n",
    "- g_pdf(x)\n",
    "- M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e3a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_sample(n_samples, g_sampler, f_unnorm, g_pdf, M, batch=10000, rng=None):\n",
    "    # Generic accept-reject sampler returning exactly n_samples accepted draws.\n",
    "    #\n",
    "    # Parameters:\n",
    "    # - n_samples: number of accepted samples to output\n",
    "    # - g_sampler(size): draws from proposal distribution g\n",
    "    # - f_unnorm(x): target density (can be unnormalized), must be >= 0\n",
    "    # - g_pdf(x): proposal density g(x), must be >= 0\n",
    "    # - M: constant with f_unnorm(x) <= M * g_pdf(x) over support\n",
    "    # - batch: batch size for vectorized proposals (speed)\n",
    "    # - rng: np.random.Generator for reproducibility\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    accepted_chunks = []\n",
    "    n_acc = 0\n",
    "\n",
    "    while n_acc < n_samples:\n",
    "        # 1) Propose candidates\n",
    "        y = g_sampler(batch)\n",
    "\n",
    "        # 2) Uniforms for accept step\n",
    "        u = rng.random(batch)\n",
    "\n",
    "        # 3) Compute acceptance probability f(y)/(M*g(y))\n",
    "        gy = g_pdf(y)\n",
    "        fy = f_unnorm(y)\n",
    "\n",
    "        # Avoid division by zero (e.g., if g_pdf(y)=0 at boundaries)\n",
    "        accept_prob = fy / (M * gy + 1e-300)\n",
    "\n",
    "        # 4) Accept\n",
    "        mask = u <= accept_prob\n",
    "        if np.any(mask):\n",
    "            accepted = y[mask]\n",
    "            accepted_chunks.append(accepted)\n",
    "            n_acc += accepted.size\n",
    "\n",
    "    # Concatenate and trim to exact size\n",
    "    return np.concatenate(accepted_chunks)[:n_samples]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f776b4d",
   "metadata": {},
   "source": [
    "### B) Inverse CDF sampling\n",
    "You provide Finv(u) = F^{-1}(u).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_cdf_sample(n_samples, Finv, rng=None):\n",
    "    # Inverse transform sampling: X = F^{-1}(U), U ~ Uniform(0,1).\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    u = rng.random(n_samples)\n",
    "    return Finv(u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83727d7b",
   "metadata": {},
   "source": [
    "### C) Monte Carlo integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddddeeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_integral_direct(samples, h):\n",
    "    # Direct MC: I = E[h(X)] \u2248 mean(h(samples)).\n",
    "    vals = h(samples)\n",
    "    return float(np.mean(vals))\n",
    "\n",
    "def mc_integral_importance(samples_from_g, h, f_pdf, g_pdf):\n",
    "    # Importance sampling: I = E_g[h(Y)*f(Y)/g(Y)].\n",
    "    y = samples_from_g\n",
    "    w = f_pdf(y) / (g_pdf(y) + 1e-300)\n",
    "    vals = h(y) * w\n",
    "    return float(np.mean(vals))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860dfabc",
   "metadata": {},
   "source": [
    "### D) Hoeffding CI\n",
    "LaTeX:\n",
    "\\[\n",
    "\\hat\\mu \\pm (b-a)\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c5c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hoeffding_ci(mean_estimate, n, a, b, delta=0.05):\n",
    "    # Two-sided Hoeffding CI for bounded variables in [a,b].\n",
    "    radius = (b - a) * np.sqrt(np.log(2.0 / delta) / (2.0 * n))\n",
    "    return float(mean_estimate - radius), float(mean_estimate + radius)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd08ec",
   "metadata": {},
   "source": [
    "# 2) Logistic regression + calibration + 0\u20131 loss CI\n",
    "\n",
    "## 2.1 Definitions (summary)\n",
    "\n",
    "### Logistic regression\n",
    "\\[\n",
    "P(Y=1|X)=\\sigma(\\beta_0+\\beta^\\top X),\\quad \\sigma(t)=\\frac{1}{1+e^{-t}}.\n",
    "\\]\n",
    "\n",
    "### Negative log-likelihood (cross-entropy)\n",
    "\\[\n",
    "\\mathcal{L} = -\\sum_{i=1}^n \\left[y_i\\log p_i + (1-y_i)\\log(1-p_i)\\right],\\quad p_i=\\sigma(\\beta_0+\\beta^\\top x_i).\n",
    "\\]\n",
    "\n",
    "### Calibration\n",
    "Learn a mapping from raw predicted probabilities \\(\\hat p\\) to calibrated probabilities \\(\\tilde p\\) using a separate calibration set.\n",
    "\n",
    "### 0\u20131 loss\n",
    "\\[\n",
    "\\hat L = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{\\hat y_i\\ne y_i\\},\\quad \\hat y_i=\\mathbf{1}\\{\\tilde p_i\\ge 0.5\\}.\n",
    "\\]\n",
    "Hoeffding (with a=0,b=1) gives:\n",
    "\\[\n",
    "\\hat L \\pm \\sqrt{\\frac{\\ln(2/\\delta)}{2n}}.\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Copyable formulas (plain text)\n",
    "- sigma(t)=1/(1+exp(-t))\n",
    "- p_i = sigma(beta0 + beta^T x_i)\n",
    "- NLL = -\u03a3 [ y_i log p_i + (1-y_i) log(1-p_i) ]\n",
    "- predict: yhat = 1 if p>=0.5 else 0\n",
    "- 0-1 loss = mean(yhat != y)\n",
    "- CI: loss \u00b1 sqrt(log(2/delta)/(2n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f6213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Imports for this chapter\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    # Numerically stable sigmoid: clip to avoid exp overflow.\n",
    "    t = np.clip(t, -50, 50)\n",
    "    return 1.0 / (1.0 + np.exp(-t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c1e5a",
   "metadata": {},
   "source": [
    "## 2.4 Code templates\n",
    "\n",
    "### A) Logistic NLL + training with scipy.optimize.minimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_nll(theta, X, y, l2=0.0):\n",
    "    # Negative log-likelihood for logistic regression (optionally with L2 penalty).\n",
    "    # theta[0] = intercept beta0, theta[1:] = beta vector.\n",
    "\n",
    "    beta0 = theta[0]\n",
    "    beta = theta[1:]\n",
    "\n",
    "    # Linear predictor\n",
    "    z = beta0 + X @ beta\n",
    "\n",
    "    # Probabilities\n",
    "    p = sigmoid(z)\n",
    "\n",
    "    # Avoid log(0)\n",
    "    eps = 1e-12\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "\n",
    "    # NLL (cross-entropy)\n",
    "    nll = -np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "\n",
    "    # L2 penalty on beta only (not intercept)\n",
    "    if l2 > 0:\n",
    "        nll += l2 * np.sum(beta ** 2)\n",
    "\n",
    "    return float(nll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a78adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic(X, y, method=\"CG\", l2=0.0):\n",
    "    # Fit logistic regression by minimizing NLL.\n",
    "    # Returns (theta_hat, scipy_result).\n",
    "    from scipy import optimize\n",
    "\n",
    "    n, d = X.shape\n",
    "    theta0 = np.zeros(d + 1)\n",
    "\n",
    "    obj = lambda th: logistic_nll(th, X, y, l2=l2)\n",
    "    res = optimize.minimize(obj, theta0, method=method)\n",
    "    return res.x, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66044d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba_logistic(theta, X):\n",
    "    # Predict probabilities P(Y=1|X) given theta.\n",
    "    beta0 = theta[0]\n",
    "    beta = theta[1:]\n",
    "    return sigmoid(beta0 + X @ beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02051dbe",
   "metadata": {},
   "source": [
    "### B) Calibration (DecisionTreeRegressor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f15dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tree_calibrator(p_cal, y_cal, max_depth=3, min_samples_leaf=50, random_state=0):\n",
    "    # Fit decision-tree regressor mapping raw probabilities -> calibrated probabilities.\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "    X_feat = p_cal.reshape(-1, 1)\n",
    "\n",
    "    calibrator = DecisionTreeRegressor(\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    calibrator.fit(X_feat, y_cal)\n",
    "    return calibrator\n",
    "\n",
    "def apply_calibrator(calibrator, p_raw):\n",
    "    # Apply calibrator and clip to [0,1].\n",
    "    p_cal = calibrator.predict(p_raw.reshape(-1, 1))\n",
    "    return np.clip(p_cal, 0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336af60b",
   "metadata": {},
   "source": [
    "### C) 0\u20131 loss + Hoeffding CI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad5439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_from_proba(p, threshold=0.5):\n",
    "    # Predict class labels from probabilities under equal costs.\n",
    "    return (p >= threshold).astype(int)\n",
    "\n",
    "def zero_one_loss(y_true, y_pred):\n",
    "    # 0-1 loss = mean of misclassification indicator.\n",
    "    return float(np.mean(y_true != y_pred))\n",
    "\n",
    "def hoeffding_ci_01(loss_estimate, n, delta=0.05):\n",
    "    # Hoeffding CI for 0-1 loss (bounded in [0,1]).\n",
    "    radius = np.sqrt(np.log(2.0 / delta) / (2.0 * n))\n",
    "    return float(loss_estimate - radius), float(loss_estimate + radius)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73071241",
   "metadata": {},
   "source": [
    "# 3) Markov chains (finite state)\n\n## 3.1 Definitions (summary)\n\n### Transition matrix\n$$\n\nP_{ij} = \\Pr(X_{t+1}=j \\mid X_t=i).\n\n$$\nRows sum to 1.\n\n### Irreducible\n$$\n\n\\forall i,j,\\ \\exists n\\ge1 \\text{ such that } (P^n)_{ij}>0.\n\n$$\nGraph view: strongly connected directed graph of positive edges.\n\n### Period\n$$\n\nd(i)=\\gcd\\{n\\ge1:(P^n)_{ii}>0\\}.\n\n$$\nAperiodic means period 1. In irreducible chains, all states share the same period.\n\n### Stationary distribution\n$$\n\n\\pi=\\pi P,\\quad \\sum_i\\pi_i=1.\n\n$$\nFinite + irreducible implies existence and uniqueness.\n\n### Reversibility (detailed balance)\n$$\n\n\\pi_i P_{ij} = \\pi_j P_{ji}\\ \\ \\forall i,j.\n\n$$\n\n---\n\n## 3.2 Copyable formulas (plain text)\n- Stationary: solve pi = pi*P with sum(pi)=1.\n- Period: d(i)=gcd{ n>=1 : (P^n)[i,i] > 0 }.\n- Reversible: pi_i P_ij = pi_j P_ji for all i,j.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db9e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Imports for this chapter\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1383d2f",
   "metadata": {},
   "source": [
    "## 3.4 Code templates (Markov chains)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463daeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_irreducible(P, tol=1e-15):\n",
    "    # Check irreducibility using reachability on the directed graph.\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    n = P.shape[0]\n",
    "    adj = (P > tol)\n",
    "\n",
    "    for start in range(n):\n",
    "        seen = {start}\n",
    "        stack = [start]\n",
    "        while stack:\n",
    "            i = stack.pop()\n",
    "            for j in range(n):\n",
    "                if adj[i, j] and j not in seen:\n",
    "                    seen.add(j)\n",
    "                    stack.append(j)\n",
    "        if len(seen) != n:\n",
    "            return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bff42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def periods(P, max_power=200, tol=1e-15):\n",
    "    # Compute state periods d(i) by collecting return times up to max_power.\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    n = P.shape[0]\n",
    "\n",
    "    returns = [[] for _ in range(n)]\n",
    "    Pk = np.eye(n)\n",
    "\n",
    "    for k in range(1, max_power + 1):\n",
    "        Pk = Pk @ P\n",
    "        diag = np.diag(Pk)\n",
    "        for i in range(n):\n",
    "            if diag[i] > tol:\n",
    "                returns[i].append(k)\n",
    "\n",
    "    def gcd_list(lst):\n",
    "        from math import gcd\n",
    "        g = lst[0]\n",
    "        for x in lst[1:]:\n",
    "            g = gcd(g, x)\n",
    "        return g\n",
    "\n",
    "    out = np.zeros(n, dtype=object)\n",
    "    for i in range(n):\n",
    "        out[i] = None if len(returns[i]) == 0 else gcd_list(returns[i])\n",
    "    return out\n",
    "\n",
    "def is_aperiodic(P):\n",
    "    # Aperiodic if all states have period 1 (assuming computation found returns).\n",
    "    per = periods(P)\n",
    "    if any(p is None for p in per):\n",
    "        return False\n",
    "    return all(int(p) == 1 for p in per)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44459f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_distribution(P):\n",
    "    # Solve pi = pi P with sum(pi)=1 using a linear system.\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    n = P.shape[0]\n",
    "\n",
    "    A = P.T - np.eye(n)\n",
    "    A[-1, :] = 1.0\n",
    "    b = np.zeros(n)\n",
    "    b[-1] = 1.0\n",
    "\n",
    "    pi = np.linalg.solve(A, b)\n",
    "    pi = np.clip(pi, 0.0, 1.0)\n",
    "    pi = pi / pi.sum()\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_reversible(P, pi=None, tol=1e-8):\n",
    "    # Check detailed balance pi_i P_ij == pi_j P_ji.\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    if pi is None:\n",
    "        pi = stationary_distribution(P)\n",
    "\n",
    "    lhs = pi.reshape(-1, 1) * P\n",
    "    rhs = (pi.reshape(1, -1) * P.T)\n",
    "    return bool(np.allclose(lhs, rhs, atol=tol, rtol=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d02c",
   "metadata": {},
   "source": [
    "## 3.5 Markov chain answer checklist (copy into exam text)\n1. Write $P$ and verify row sums.\n2. Irreducible? argue reachability / strongly connected.\n3. Periods: compute gcd of return times; self-loop implies period 1.\n4. Stationary: solve $\\pi=\\pi P$, $\\sum\\pi=1$.\n5. Reversible: check detailed balance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Detailed Markov chain explanation with worked example (copy-friendly)\n\n### What is a Markov chain?\nA **(discrete-time) Markov chain** is a stochastic process $(X_t)_{t\\ge 0}$ taking values in a finite set of states\n$S=\\{1,2,\\dots,n\\}$ such that the **Markov property** holds:\n\nPlain text:\n- P(X_{t+1}=j | X_t=i, X_{t-1},...,X_0) = P(X_{t+1}=j | X_t=i) = P_ij\n\nLaTeX:\n$$\n\n\\Pr(X_{t+1}=j \\mid X_t=i, X_{t-1},\\dots,X_0)=\\Pr(X_{t+1}=j \\mid X_t=i)=P_{ij}.\n\n$$\n\nInterpretation:\n- \u201cThe future depends only on the present, not on the full history.\u201d\n\n---\n\n### Q1. Transition matrix\nDefinition:\n- The **transition matrix** $P\\in\\mathbb{R}^{n\\times n}$ has entries $P_{ij}=\\Pr(X_{t+1}=j\\mid X_t=i)$.\n\nPlain text:\n- Each row i lists probabilities of moving out of state i.\n- Rows sum to 1 and entries are \u2265 0.\n\nLaTeX:\n$$\n\nP_{ij}=\\Pr(X_{t+1}=j\\mid X_t=i),\\qquad \\sum_{j=1}^n P_{ij}=1,\\; P_{ij}\\ge 0.\n\n$$\n\nHow to build from a diagram:\n1. Fix an ordering of states (A,B,C,...) \u2192 indices (0,1,2,...).\n2. For each state, read outgoing arrows and probabilities.\n3. Fill row i accordingly (use 0 where no arrow exists).\n\n---\n\n### Q2. Irreducible?\nDefinitions:\n- State j is **reachable** from i if there exists $k\\ge 1$ with $(P^k)_{ij}>0$.\n- i and j **communicate** if i reaches j and j reaches i.\n- The chain is **irreducible** if every pair of states communicates (one communicating class).\n\nPlain text quick check:\n- In the state graph, can you get from every node to every other node following directed edges?\n- If yes \u2192 irreducible. If it splits into separate \u201cclosed\u201d parts \u2192 reducible.\n\nLaTeX:\n$$\n\ni\\to j \\iff \\exists k\\ge 1:\\ (P^k)_{ij}>0,\\qquad\n\\text{irreducible} \\iff \\forall i,j:\\ i\\to j.\n\n$$\n\n---\n\n### Q3. Aperiodic? Period of each state\nReturn times:\n- Look at the set $T_i=\\{k\\ge 1 : (P^k)_{ii}>0\\}$, i.e., times when you can return to i.\n\nPeriod of state i:\nPlain text:\n- d(i) = gcd of all return times to i.\n\nLaTeX:\n$$\n\nd(i)=\\gcd\\{k\\ge 1:\\ (P^k)_{ii}>0\\}.\n\n$$\n\nAperiodic:\n- State i is **aperiodic** if $d(i)=1$.\n- In a **finite irreducible** chain, all states have the same period, so it\u2019s enough to find one state with period 1.\n\nFast exam rules:\n- If any state has a self-loop $P_{ii}>0$, then that state has period 1.\n- In an irreducible chain, a single self-loop implies the whole chain is aperiodic.\n\n---\n\n### Q4. Stationary distribution (exists? what is it?)\nDefinition:\n- A probability vector $\\pi$ is **stationary** if $\\pi=\\pi P$.\n\nPlain text:\n- If X_0 ~ \u03c0, then X_t ~ \u03c0 for all t.\n\nLaTeX:\n$$\n\n\\pi=\\pi P,\\qquad \\sum_{i=1}^n \\pi_i=1,\\ \\pi_i\\ge 0.\n\n$$\n\nExistence/uniqueness (finite case):\n- If the chain is **irreducible** and finite \u2192 a stationary distribution exists and is unique.\n- If also **aperiodic**, then $P^t$ converges to \u03c0 (long-run convergence).\n\nHow to compute:\n1. Solve the linear system $\\pi=\\pi P$.\n2. Add the normalization constraint $\\sum_i \\pi_i=1$.\n3. (Practical) Replace one equation in $\\pi=\\pi P$ with the normalization to avoid singularity.\n\n---\n\n### Q5. Reversible?\nDefinition (detailed balance):\n- A chain is **reversible** w.r.t. \u03c0 if for all i,j:\n  \u03c0_i P_ij = \u03c0_j P_ji.\n\nLaTeX:\n$$\n\n\\pi_i P_{ij}=\\pi_j P_{ji}\\quad \\forall i,j.\n\n$$\n\nHow to check fast:\n1. Find \u03c0 (stationary distribution).\n2. Check detailed balance only on edges (pairs where $P_{ij}>0$ or $P_{ji}>0$).\n3. One violation \u2192 not reversible.\n\n---\n\n## Worked example (3 states)\nSuppose states are (A,B,C) and transitions are:\n- From A: A\u2192A with 0.2, A\u2192B with 0.8\n- From B: B\u2192A with 0.5, B\u2192C with 0.5\n- From C: C\u2192B with 1.0\n\n### Transition matrix\n$$\n\nP=\n\\begin{pmatrix}\n0.2 & 0.8 & 0\\\\\n0.5 & 0 & 0.5\\\\\n0 & 1.0 & 0\n\\end{pmatrix}.\n\n$$\n\n### Irreducible?\n- A reaches C via A\u2192B\u2192C.\n- C reaches A via C\u2192B\u2192A.\n- All states reach each other \u2192 **irreducible**.\n\n### Periods / aperiodic?\n- A has self-loop (P_AA=0.2>0) \u21d2 d(A)=1.\n- Irreducible \u21d2 all states have period 1.\nSo the chain is **aperiodic**.\n\n### Stationary distribution\nSolve $\\pi=\\pi P$ with $\\pi_A+\\pi_B+\\pi_C=1$. The solution is:\n$$\n\n\\pi=\\left(\\frac{5}{17},\\frac{8}{17},\\frac{4}{17}\\right).\n\n$$\n\n### Reversibility\nCheck detailed balance on edges:\n- A\u2194B: $\\pi_A P_{AB}=(5/17)\\cdot 0.8=4/17$, $\\pi_B P_{BA}=(8/17)\\cdot 0.5=4/17$.\n- B\u2194C: $\\pi_B P_{BC}=(8/17)\\cdot 0.5=4/17$, $\\pi_C P_{CB}=(4/17)\\cdot 1=4/17$.\nAll checks pass \u21d2 **reversible**.\n\n---\n\n### Mini answer template (paste into exam)\nPlain text:\n1) Transition matrix: write P with stated order, verify rows sum to 1.\n2) Irreducible: argue reachability/one communicating class (or show a closed class).\n3) Periods: compute gcd of return times; self-loop \u21d2 period 1; if irreducible then all same.\n4) Stationary: solve pi = pi P + sum(pi)=1; exists uniquely if finite + irreducible.\n5) Reversible: check detailed balance pi_i P_ij = pi_j P_ji on edges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f5726",
   "metadata": {},
   "source": [
    "# Appendix: sanity checks (optional)\nRun during preparation to verify helper functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f21ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_example = np.array([[0.9, 0.1],\n",
    "                      [0.4, 0.6]])\n",
    "\n",
    "print(\"Irreducible:\", is_irreducible(P_example))\n",
    "print(\"Periods:\", periods(P_example))\n",
    "pi_ex = stationary_distribution(P_example)\n",
    "print(\"Stationary pi:\", pi_ex, \"sum=\", pi_ex.sum())\n",
    "print(\"Reversible:\", is_reversible(P_example, pi_ex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LaTeX-safe exam template (copy/paste)\n\nUse **$...$** for inline math and **$$...$$** for display math.  \nAvoid relying on `\\(...\\)` / `\\[...\\]` because some viewers don\u2019t render them.\n\n### Definitions (copy-friendly)\n**Plain text:**  \n- State space: S = {1,2,...,n}  \n- Transition matrix entry: P_ij = P(X_{t+1}=j | X_t=i)\n\n**LaTeX (inline):**  \n- State space: $S=\\{1,2,\\dots,n\\}$  \n- Entry: $P_{ij}=\\Pr(X_{t+1}=j\\mid X_t=i)$\n\n### One key equation (display)\n$$\n\\pi = \\pi P, \\qquad \\sum_{i=1}^n \\pi_i = 1, \\qquad \\pi_i\\ge 0.\n$$\n\n### Short reasoning template\n- **Step 1 (matrix):** \u201cOrder states as (A,B,...) and fill each row with outgoing probabilities; rows sum to 1.\u201d  \n- **Step 2 (irreducible):** \u201cCheck reachability/strong connectivity in the directed graph.\u201d  \n- **Step 3 (period):** \u201cCompute $d(i)=\\gcd\\{n\\ge 1:(P^n)_{ii}>0\\}$; self-loop implies $d(i)=1$.\u201d  \n- **Step 4 (stationary):** \u201cSolve $\\pi=\\pi P$ plus normalization.\u201d  \n- **Step 5 (reversible):** \u201cCheck detailed balance $\\pi_iP_{ij}=\\pi_jP_{ji}$ on each edge.\u201d\n\n### Common pitfalls\n- Writing math outside $...$ / $$...$$ (won\u2019t render).\n- Putting math inside code blocks ```...``` (won\u2019t render).\n- Forgetting the normalization equation for $\\pi$.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}